{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dba5647",
   "metadata": {},
   "source": [
    "#### Day 65 of Python Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca75780e",
   "metadata": {},
   "source": [
    "## Introduction to Web Scraping\n",
    "\n",
    "What is Web Scraping?\n",
    "\n",
    "Web scraping is the process of extracting data from websites. Instead of manually copying and pasting information, web scraping automates the task using a program. The extracted data can be stored for further analysis, visualization, or application development.\n",
    "\n",
    "### Key Use Cases of Web Scraping:\n",
    "\n",
    "Market Research: Extract product details and prices from e-commerce websites.\n",
    "\n",
    "Content Aggregation: Collect news articles, blogs, or public data.\n",
    "\n",
    "Sentiment Analysis: Gather user reviews from social media or forums."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c79a2ee",
   "metadata": {},
   "source": [
    "### Legal and Ethical Considerations\n",
    "\n",
    "Before you start web scraping, it is crucial to ensure you are doing it legally and ethically:\n",
    "\n",
    "Check robots.txt: Websites use a robots.txt file to indicate the pages that can or cannot be scraped. You can find this file by appending /robots.txt to a website’s URL (e.g., https://example.com/robots.txt).\n",
    "\n",
    "Respect Terms of Service: Review the website’s terms and conditions to understand its data usage policies.\n",
    "\n",
    "Avoid Overloading the Server: Make requests responsibly to avoid harming the website’s infrastructure.\n",
    "\n",
    "### Setting Up the Environment\n",
    "\n",
    "Required Libraries\n",
    "\n",
    "For this tutorial, we will use two Python libraries:\n",
    "\n",
    "requests: To send HTTP requests and fetch web pages.\n",
    "\n",
    "Beautiful Soup: To parse and navigate the HTML structure.\n",
    "\n",
    "Installation\n",
    "\n",
    "To install the libraries, run the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46240502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\tobun\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\tobun\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tobun\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tobun\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tobun\\anaconda3\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tobun\\anaconda3\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\tobun\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa90086",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33df4885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a897154",
   "metadata": {},
   "source": [
    "### Step 2: Send a Request to the Webpage\n",
    "\n",
    "Choose a webpage you want to scrape (e.g., https://example.com). Use the requests library to fetch its content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a17cf189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page fetched successfully!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://example.com\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check the status of the request\n",
    "if response.status_code == 200:\n",
    "    print(\"Page fetched successfully!\")\n",
    "else:\n",
    "    print(\"Failed to fetch the page. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee27936",
   "metadata": {},
   "source": [
    "### Step 3: Parse the HTML Content\n",
    "\n",
    "Use Beautiful Soup to parse the content of the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90ba1320",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e7e844",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "\n",
    "requests.get() fetches the web page's HTML.\n",
    "\n",
    "BeautifulSoup(response.text, \"html.parser\") parses the HTML content.\n",
    "\n",
    "\n",
    ".prettify() outputs neatly formatted HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261db13a",
   "metadata": {},
   "source": [
    "### Step 4: Extract Specific Elements\n",
    "\n",
    "Extract the page title:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a691b3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Title: Example Domain\n"
     ]
    }
   ],
   "source": [
    "title = soup.title.string\n",
    "print(\"Page Title:\", title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d6b30c",
   "metadata": {},
   "source": [
    "#### Extract the first paragraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b75765f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Paragraph: This domain is for use in illustrative examples in documents. You may use this\n",
      "    domain in literature without prior coordination or asking for permission.\n"
     ]
    }
   ],
   "source": [
    "first_paragraph = soup.find(\"p\").text\n",
    "print(\"First Paragraph:\", first_paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c7dfd0",
   "metadata": {},
   "source": [
    "#### Accessing attributes of a tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "519cc19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Href attribute: https://www.iana.org/domains/example\n"
     ]
    }
   ],
   "source": [
    "link = soup.find('a')  # Finds the first <a> tag\n",
    "print(\"Href attribute:\", link['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b68fd3f",
   "metadata": {},
   "source": [
    "####  Find all elements of a specific tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcd9ce47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.iana.org/domains/example\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_links = soup.find_all('a')  # Find all <a> tags\n",
    "for link in all_links:\n",
    "    print(link['href'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a7875a",
   "metadata": {},
   "source": [
    "### Saving Extracted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93c9aba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Links saved to links.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Example: Save extracted links to a CSV\n",
    "links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "\n",
    "with open(\"links.csv\", \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Links\"])\n",
    "    for link in links:\n",
    "        writer.writerow([link])\n",
    "\n",
    "print(\"Links saved to links.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2338b983",
   "metadata": {},
   "source": [
    "### Combining requests and BeautifulSoup\n",
    "\n",
    "To scrape data from a real web page, combine requests to fetch the page and BeautifulSoup to parse the HTML.\n",
    "\n",
    "Example: Scraping a Real Web Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2876a933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title of the page: Example Domain\n",
      "Heading text: Example Domain\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the page you want to scrape\n",
    "url = 'https://www.example.com'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Create a BeautifulSoup object\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract and print the title of the page\n",
    "    title = soup.title.string\n",
    "    print(f\"Title of the page: {title}\")\n",
    "    \n",
    "    # Extract and print all headings (h1, h2, h3)\n",
    "    headings = soup.find_all(['h1', 'h2', 'h3'])\n",
    "    for heading in headings:\n",
    "        print(f\"Heading text: {heading.get_text()}\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve page. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fe0cc6",
   "metadata": {},
   "source": [
    "### Understanding the Code\n",
    "\n",
    "requests.get(url): Sends an HTTP GET request to fetch the webpage.\n",
    "\n",
    "BeautifulSoup(response.content, \"html.parser\"): Parses the HTML content of the page.\n",
    "\n",
    "soup.title.string: Retrieves the title of the page.\n",
    "\n",
    "soup.find(\"p\"): Finds the first paragraph (<p> tag) on the page.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3347dda4",
   "metadata": {},
   "source": [
    "#### Assignment\n",
    "\n",
    "Choose a website of your choice and:\n",
    "\n",
    "Extract the title of the page.\n",
    "\n",
    "Extract the first three headings (e.g., h1, h2 tags).\n",
    "\n",
    "Extract the links (URLs) from the page.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822b6880",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
